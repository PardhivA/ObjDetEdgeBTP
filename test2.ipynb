{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip 'Tiny ImageNet dataset.zip' -d tiny-imagenet-another/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any class folder in tiny-imagenet-another/tiny-imagenet-200/val/images.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m      7\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      8\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),  \u001b[38;5;66;03m# Resize to EfficientNet's expected input size\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     10\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m]),  \u001b[38;5;66;03m# ImageNet normalization\u001b[39;00m\n\u001b[1;32m     11\u001b[0m ])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load the Tiny ImageNet validation dataset\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m val_data \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtiny-imagenet-another/tiny-imagenet-200/val/images\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Load the pretrained EfficientNet model\u001b[39;00m\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/torchvision/datasets/folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    321\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    327\u001b[0m ):\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/torchvision/datasets/folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[1;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/torchvision/datasets/folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/torchvision/datasets/folder.py:43\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     41\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mscandir(directory) \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m class_to_idx \u001b[38;5;241m=\u001b[39m {cls_name: i \u001b[38;5;28;01mfor\u001b[39;00m i, cls_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(classes)}\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m classes, class_to_idx\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in tiny-imagenet-another/tiny-imagenet-200/val/images."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to EfficientNet's expected input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet normalization\n",
    "])\n",
    "\n",
    "# Load the Tiny ImageNet validation dataset\n",
    "val_data = datasets.ImageFolder(root='tiny-imagenet-another/tiny-imagenet-200/val/images', transform=transform)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load the pretrained EfficientNet model\n",
    "model = models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "# Modify the classifier to match the Tiny ImageNet (200 classes)\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 200)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Evaluate the model\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = (correct / total) * 100\n",
    "print(f'Accuracy on Tiny ImageNet validation set: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install efficientnet_pytorch pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 625/625 [01:06<00:00,  9.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 5.3179, Test Accuracy: 0.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformations for testing\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the WordNet to Label Mapping\n",
    "def load_wordnet_mappings(mapping_file):\n",
    "    wordnet_to_label = {}\n",
    "    with open(mapping_file, 'r') as f:\n",
    "        for line in f:\n",
    "            wordnet_id, label = line.strip().split('\\t')\n",
    "            wordnet_to_label[wordnet_id] = label\n",
    "    return wordnet_to_label\n",
    "\n",
    "# Custom Dataset class for Tiny ImageNet validation data\n",
    "class TinyImageNetDataset(Dataset):\n",
    "    def __init__(self, img_folder, annotations_file, transform=None):\n",
    "        self.img_folder = img_folder\n",
    "        self.annotations = pd.read_csv(annotations_file, sep='\\t', header=None, names=['image', 'class', 'x1', 'y1', 'x2', 'y2'])\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Mapping class labels to integers (since the class ids in the annotations might be in string format)\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(sorted(self.annotations['class'].unique()))}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.annotations.iloc[idx, 0]\n",
    "        img_path = os.path.join(self.img_folder, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Get the label from the annotations\n",
    "        class_name = self.annotations.iloc[idx, 1]\n",
    "        label = self.class_to_idx[class_name]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Load the validation dataset\n",
    "img_folder = 'tiny-imagenet-another/tiny-imagenet-200/val/images'  # Update with the path to the 'val_images' folder\n",
    "annotations_file = 'tiny-imagenet-another/tiny-imagenet-200/val/val_annotations.txt'  # Update with the path to the annotations file\n",
    "wordnet_mapping_file = 'tiny-imagenet-another/tiny-imagenet-200/words.txt'  # Path to the WordNet to label mapping file\n",
    "\n",
    "# Load WordNet mappings\n",
    "wordnet_to_label = load_wordnet_mappings(wordnet_mapping_file)\n",
    "\n",
    "# Initialize the dataset and DataLoader\n",
    "test_dataset = TinyImageNetDataset(img_folder=img_folder, annotations_file=annotations_file, transform=test_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "# Load pre-trained EfficientNet model\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=200)  # Tiny ImageNet has 200 classes\n",
    "model = model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define loss function (optional, just for reporting the loss)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Evaluate the model\n",
    "# Evaluate the model\n",
    "def evaluate(model, test_loader, criterion, device, wordnet_to_label):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Map predicted indices to WordNet labels\n",
    "            predicted_classes = [wordnet_to_label.get(test_loader.dataset.annotations.iloc[i, 1], 'Unknown') for i in predicted.cpu().numpy()]\n",
    "            # print(f\"Predicted classes: {predicted_classes}\")\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    avg_loss = running_loss / len(test_loader)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "test_loss, test_accuracy = evaluate(model, test_loader, criterion, device, wordnet_to_label)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# class TinyImageNetDataset(Dataset):\n",
    "#     def __init__(self, root_dir, transform=None):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.transform = transform\n",
    "#         self.annotations = self._load_annotations()\n",
    "\n",
    "#     def _load_annotations(self):\n",
    "#         annotations = []\n",
    "#         for folder in os.listdir(self.root_dir):\n",
    "#             folder_path = os.path.join(self.root_dir, folder)\n",
    "#             if os.path.isdir(folder_path):\n",
    "#                 image_folder = os.path.join(folder_path, 'images')\n",
    "#                 boxes_file = os.path.join(folder_path, f'{folder}_boxes.txt')\n",
    "                \n",
    "#                 # The folder name represents the class_id\n",
    "#                 class_id = folder\n",
    "                \n",
    "#                 if os.path.exists(boxes_file):\n",
    "#                     with open(boxes_file, 'r') as f:\n",
    "#                         lines = f.readlines()\n",
    "#                     for line in lines:\n",
    "#                         parts = line.split()\n",
    "#                         if len(parts) == 5:  # Ensure there are exactly 5 elements\n",
    "#                             img_name, xmin, ymin, xmax, ymax = parts\n",
    "#                             img_path = os.path.join(image_folder, img_name)\n",
    "#                             annotations.append((img_path, class_id, int(xmin), int(ymin), int(xmax), int(ymax)))  # store bbox\n",
    "#         return annotations\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.annotations)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path, class_id, xmin, ymin, xmax, ymax = self.annotations[idx]\n",
    "#         img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "#         if self.transform:\n",
    "#             img = self.transform(img)\n",
    "        \n",
    "#         return img, class_id, xmin, ymin, xmax, ymax  # Return bbox as well if needed\n",
    "\n",
    "\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class TinyImageNetDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = self._get_class_to_idx()  # Map class names to integers\n",
    "        self.annotations = self._load_annotations()\n",
    "\n",
    "    def _get_class_to_idx(self):\n",
    "        \"\"\"\n",
    "        Maps folder names (class IDs) to integer labels.\n",
    "        \"\"\"\n",
    "        classes = sorted(os.listdir(self.root_dir))\n",
    "        class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "        return class_to_idx\n",
    "\n",
    "    def _load_annotations(self):\n",
    "        annotations = []\n",
    "        for folder in os.listdir(self.root_dir):\n",
    "            folder_path = os.path.join(self.root_dir, folder)\n",
    "            if os.path.isdir(folder_path):\n",
    "                image_folder = os.path.join(folder_path, 'images')\n",
    "                boxes_file = os.path.join(folder_path, f'{folder}_boxes.txt')\n",
    "                \n",
    "                # Get the class_id as an integer from the mapping\n",
    "                class_id = self.class_to_idx[folder]\n",
    "                \n",
    "                if os.path.exists(boxes_file):\n",
    "                    with open(boxes_file, 'r') as f:\n",
    "                        lines = f.readlines()\n",
    "                    for line in lines:\n",
    "                        parts = line.split()\n",
    "                        if len(parts) == 5:  # Ensure there are exactly 5 elements in the line\n",
    "                            img_name, xmin, ymin, xmax, ymax = parts\n",
    "                            img_path = os.path.join(image_folder, img_name)\n",
    "                            annotations.append((img_path, class_id, int(xmin), int(ymin), int(xmax), int(ymax)))  # store bbox\n",
    "        return annotations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_id, xmin, ymin, xmax, ymax = self.annotations[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # Return image and class_id only for classification purposes\n",
    "        return img, class_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define your transformations for data augmentation and preprocessing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Initialize Dataset and DataLoader\n",
    "train_dataset = TinyImageNetDataset(root_dir='/home/btp_9/EfficientNet/tiny-imagenet-another/tiny-imagenet-200/train', transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# Load pre-trained EfficientNet-B0 model\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "\n",
    "# Replace the final fully connected layer for 200 classes\n",
    "model._fc = torch.nn.Linear(model._fc.in_features, 200)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Freeze all layers except the final layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last fully connected layer\n",
    "for param in model._fc.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 6250/6250 [12:43<00:00,  8.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Loss: 4.0796, Accuracy: 29.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 6250/6250 [12:42<00:00,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Loss: 2.9645, Accuracy: 40.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Fine-tuning loop\n",
    "def fine_tune(model, train_loader, criterion, optimizer, device, epochs=5):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)  # Move images and labels to device\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update running statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Train the model\n",
    "fine_tune(model, train_loader, criterion, optimizer, device, epochs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "torch.save(model.state_dict(), 'fine_tuned_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyImageNetValidationDataset(Dataset):\n",
    "    def __init__(self, root_dir, annotations_file, class_to_idx, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.annotations = self._load_annotations(annotations_file)\n",
    "\n",
    "    def _load_annotations(self, annotations_file):\n",
    "        annotations = []\n",
    "        with open(annotations_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 2:\n",
    "                img_name = parts[0]\n",
    "                class_id = self.class_to_idx[parts[1]]  # Map WordNet ID to integer class ID\n",
    "                img_path = os.path.join(self.root_dir, 'images', img_name)\n",
    "                annotations.append((img_path, class_id))\n",
    "        return annotations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_id = self.annotations[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, class_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create WordNet ID to integer class mapping\n",
    "class_to_idx = {wnid: idx for idx, wnid in enumerate(sorted(os.listdir(train_dataset.root_dir)))}\n",
    "\n",
    "val_dataset = TinyImageNetValidationDataset(\n",
    "    root_dir='/home/btp_9/EfficientNet/tiny-imagenet-another/tiny-imagenet-200/val',\n",
    "    annotations_file='/home/btp_9/EfficientNet/tiny-imagenet-another/tiny-imagenet-200/val/val_annotations.txt',\n",
    "    class_to_idx=class_to_idx,\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyImageNetTestDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = sorted([os.path.join(root_dir, fname) for fname in os.listdir(root_dir) if fname.endswith('.JPEG')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, img_path  # Return image and its path for identification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_601977/3332869965.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('fine_tuned_model.pth'))\n",
      "Testing: 100%|██████████| 625/625 [01:08<00:00,  9.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 54.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the test dataset and loader\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# Load the fine-tuned model for testing\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "model._fc = torch.nn.Linear(model._fc.in_features, 200)  # Ensure the output layer is correct\n",
    "model.load_state_dict(torch.load('fine_tuned_model.pth'))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate(model, test_loader, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Test the model\n",
    "evaluate(model, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, data_loader, device):\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for images, img_paths in tqdm(data_loader, desc=\"Testing\"):\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            for img_path, pred in zip(img_paths, predicted):\n",
    "                results.append((img_path, pred.item()))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 625/625 [01:07<00:00,  9.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: /home/btp_9/EfficientNet/tiny-imagenet-another/tiny-imagenet-200/test/images/test_0.JPEG, Predicted Class: 130\n",
      "Image: /home/btp_9/EfficientNet/tiny-imagenet-another/tiny-imagenet-200/test/images/test_1.JPEG, Predicted Class: 179\n",
      "Image: /home/btp_9/EfficientNet/tiny-imagenet-another/tiny-imagenet-200/test/images/test_10.JPEG, Predicted Class: 178\n",
      "Image: /home/btp_9/EfficientNet/tiny-imagenet-another/tiny-imagenet-200/test/images/test_100.JPEG, Predicted Class: 159\n",
      "Image: /home/btp_9/EfficientNet/tiny-imagenet-another/tiny-imagenet-200/test/images/test_1000.JPEG, Predicted Class: 26\n",
      "Image: /home/btp_9/EfficientNet/tiny-imagenet-another/tiny-imagenet-200/test/images/test_1001.JPEG, Predicted Class: 112\n",
      "Image: /home/btp_9/EfficientNet/tiny-imagenet-another/tiny-imagenet-200/test/images/test_1002.JPEG, Predicted Class: 113\n",
      "Image: /home/btp_9/EfficientNet/tiny-imagenet-another/tiny-imagenet-200/test/images/test_1003.JPEG, Predicted Class: 53\n",
      "Image: /home/btp_9/EfficientNet/tiny-imagenet-another/tiny-imagenet-200/test/images/test_1004.JPEG, Predicted Class: 105\n",
      "Image: /home/btp_9/EfficientNet/tiny-imagenet-another/tiny-imagenet-200/test/images/test_1005.JPEG, Predicted Class: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TinyImageNetTestDataset(root_dir='/home/btp_9/EfficientNet/tiny-imagenet-another/tiny-imagenet-200/test/images', transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "test_results = inference(model, test_loader, device)\n",
    "\n",
    "# Print some results\n",
    "for img_path, pred in test_results[:10]:\n",
    "    print(f\"Image: {img_path}, Predicted Class: {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TinyImageNetTestDataset(root_dir='/home/btp_9/EfficientNet/tiny-imagenet-another/tiny-imagenet-200/test/images', transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_601977/4120950662.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('fine_tuned_model.pth'))\n",
      "Calibrating Model: 100%|██████████| 625/625 [02:32<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Size: 17.35 MB\n",
      "Quantized Model Size: 16.62 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Load Pretrained and Fine-Tuned Model\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "model._fc = torch.nn.Linear(model._fc.in_features, 200)  # 200 output classes\n",
    "model.load_state_dict(torch.load('fine_tuned_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Add QuantStub and DeQuantStub for Static Quantization\n",
    "class QuantizedEfficientNet(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(QuantizedEfficientNet, self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.model = model\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.model(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "quantized_model = QuantizedEfficientNet(model)\n",
    "\n",
    "# Step 3: Fuse Layers (No fusing is required here as EfficientNet already includes fused layers)\n",
    "# For other models, you might need to fuse Conv2d + BatchNorm + ReLU, etc.\n",
    "\n",
    "# Step 4: Prepare the Model for Static Quantization\n",
    "quantized_model.eval()\n",
    "quantized_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')  # Use 'fbgemm' for x86 CPUs\n",
    "\n",
    "# Prepare the model for static quantization\n",
    "torch.quantization.prepare(quantized_model, inplace=True)\n",
    "\n",
    "# Step 5: Calibrate the Model Using Representative Dataset\n",
    "# Use a subset of your validation or training dataset for calibration\n",
    "def calibrate(model, loader):\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(loader, desc=\"Calibrating Model\"):\n",
    "            model(images)\n",
    "\n",
    "# Example data loader for calibration\n",
    "calibration_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)  # Adjust dataset as needed\n",
    "calibrate(quantized_model, calibration_loader)\n",
    "\n",
    "# Step 6: Convert to a Quantized Model\n",
    "torch.quantization.convert(quantized_model, inplace=True)\n",
    "\n",
    "# Step 7: Save and Compare Model Sizes\n",
    "torch.save(quantized_model.state_dict(), 'quantized_model.pth')\n",
    "\n",
    "original_size = os.path.getsize('fine_tuned_model.pth')\n",
    "quantized_size = os.path.getsize('quantized_model.pth')\n",
    "\n",
    "print(f\"Original Model Size: {original_size / 1e6:.2f} MB\")\n",
    "print(f\"Quantized Model Size: {quantized_size / 1e6:.2f} MB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Quantized Model:   0%|          | 0/625 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::_slow_conv2d_forward' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_slow_conv2d_forward' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:30476 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44679 [kernel]\nMeta: registered at ../aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:96 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:17004 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:321 [backend fallback]\nAutocastXPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:463 [backend fallback]\nAutocastMPS: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Use the test loader to evaluate the quantized model\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mevaluate_static_quantized_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantized_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[76], line 11\u001b[0m, in \u001b[0;36mevaluate_static_quantized_model\u001b[0;34m(model, test_loader, class_names)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(test_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating Quantized Model\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m         predictions\u001b[38;5;241m.\u001b[39mextend(predicted\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[74], line 24\u001b[0m, in \u001b[0;36mQuantizedEfficientNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant(x)\n\u001b[0;32m---> 24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdequant(x)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/efficientnet_pytorch/model.py:314\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"EfficientNet's forward function.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m   Calls extract_features to extract features, applies final linear layer, and returns logits.\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03m    Output of this model after processing.\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# Convolution layers\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Pooling and final linear layer\u001b[39;00m\n\u001b[1;32m    316\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_avg_pooling(x)\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/efficientnet_pytorch/model.py:289\u001b[0m, in \u001b[0;36mEfficientNet.extract_features\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"use convolution layer to extract feature .\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    layer in the efficientnet model.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Stem\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swish(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bn0(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_stem\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# Blocks\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocks):\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/efficientnet_pytorch/utils.py:275\u001b[0m, in \u001b[0;36mConv2dStaticSamePadding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    274\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_padding(x)\n\u001b[0;32m--> 275\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::_slow_conv2d_forward' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_slow_conv2d_forward' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:30476 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44679 [kernel]\nMeta: registered at ../aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:96 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:17004 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:321 [backend fallback]\nAutocastXPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:463 [backend fallback]\nAutocastMPS: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Evaluate the Static Quantized Model\n",
    "def evaluate_static_quantized_model(model, test_loader, class_names=None):\n",
    "    model.eval()\n",
    "    model.to('cpu')\n",
    "    predictions = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating Quantized Model\"):\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "            # Calculate accuracy if labels are available\n",
    "            if labels is not None:\n",
    "                correct += (predicted.cpu() == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "    if total > 0:\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    else:\n",
    "        print(\"No labels available for accuracy calculation.\")\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Use the test loader to evaluate the quantized model\n",
    "evaluate_static_quantized_model(quantized_model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Quantization using torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in ./venv311/lib/python3.11/site-packages (0.20.1)\n",
      "Requirement already satisfied: numpy in ./venv311/lib/python3.11/site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: torch==2.5.1 in ./venv311/lib/python3.11/site-packages (from torchvision) (2.5.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv311/lib/python3.11/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: filelock in ./venv311/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv311/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./venv311/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./venv311/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./venv311/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./venv311/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./venv311/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./venv311/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./venv311/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./venv311/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./venv311/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./venv311/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./venv311/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./venv311/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./venv311/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./venv311/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./venv311/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./venv311/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./venv311/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv311/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv311/lib/python3.11/site-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/btp_9/EfficientNet/venv311/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/btp_9/EfficientNet/venv311/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /home/btp_9/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
      "100%|██████████| 20.5M/20.5M [00:00<00:00, 49.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = efficientnet_b0(pretrained=True)\n",
    "\n",
    "# Replace the classifier for 200 classes\n",
    "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 200)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (2): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
      "      )\n",
      "      (3): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (8): Conv2dNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=True)\n",
      "    (1): Linear(in_features=1280, out_features=200, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "did not find fuser method for: (<class 'torchvision.ops.misc.Conv2dNormActivation'>, <class 'torch.nn.modules.container.Sequential'>, <class 'torch.nn.modules.container.Sequential'>) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m             fuse_modules(module, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Fuse Conv2d, BatchNorm2d, ReLU\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mfuse_efficientnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[80], line 7\u001b[0m, in \u001b[0;36mfuse_efficientnet\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module_name, module \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential):\n\u001b[0;32m----> 7\u001b[0m         \u001b[43mfuse_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fuse Conv2d, BatchNorm2d, ReLU\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/torch/ao/quantization/fuse_modules.py:193\u001b[0m, in \u001b[0;36mfuse_modules\u001b[0;34m(model, modules_to_fuse, inplace, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfuse_modules\u001b[39m(\n\u001b[1;32m    133\u001b[0m     model,\n\u001b[1;32m    134\u001b[0m     modules_to_fuse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m     fuse_custom_config_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    138\u001b[0m ):\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Fuse a list of modules into a single module.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    Fuses only the following sequence of modules:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m \n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fuse_modules\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodules_to_fuse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_qat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuser_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuser_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuse_custom_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuse_custom_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/torch/ao/quantization/fuse_modules.py:120\u001b[0m, in \u001b[0;36m_fuse_modules\u001b[0;34m(model, modules_to_fuse, is_qat, inplace, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[1;32m    116\u001b[0m     model \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(model)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(module_element, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m module_element \u001b[38;5;129;01min\u001b[39;00m modules_to_fuse):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# Handle case of modules_to_fuse being a list\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     \u001b[43m_fuse_modules_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodules_to_fuse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_qat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuser_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse_custom_config_dict\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# Handle case of modules_to_fuse being a list of lists\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module_list \u001b[38;5;129;01min\u001b[39;00m modules_to_fuse:\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/torch/ao/quantization/fuse_modules.py:100\u001b[0m, in \u001b[0;36m_fuse_modules_helper\u001b[0;34m(model, modules_to_fuse, is_qat, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[1;32m     97\u001b[0m     mod_list\u001b[38;5;241m.\u001b[39mappend(_get_module(model, item))\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Fuse list of modules\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m new_mod_list \u001b[38;5;241m=\u001b[39m \u001b[43mfuser_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_qat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_fuser_method_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Replace original module list with fused module list\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules_to_fuse):\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/torch/ao/quantization/fuse_modules.py:59\u001b[0m, in \u001b[0;36mfuse_known_modules\u001b[0;34m(mod_list, is_qat, additional_fuser_method_mapping)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of known fuse modules.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03mReturns a list of modules that fuses the operations specified\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mthe fused operation. The rest of the elements are set to nn.Identity()\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(type_before_parametrizations(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m mod_list)\n\u001b[0;32m---> 59\u001b[0m fuser_method \u001b[38;5;241m=\u001b[39m \u001b[43mget_fuser_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_fuser_method_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fuser_method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot fuse modules: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/torch/ao/quantization/fuser_method_mappings.py:226\u001b[0m, in \u001b[0;36mget_fuser_method\u001b[0;34m(op_list, additional_fuser_method_mapping)\u001b[0m\n\u001b[1;32m    222\u001b[0m all_mappings \u001b[38;5;241m=\u001b[39m get_combined_dict(\n\u001b[1;32m    223\u001b[0m     _DEFAULT_OP_LIST_TO_FUSER_METHOD, additional_fuser_method_mapping\n\u001b[1;32m    224\u001b[0m )\n\u001b[1;32m    225\u001b[0m fuser_method \u001b[38;5;241m=\u001b[39m all_mappings\u001b[38;5;241m.\u001b[39mget(op_list, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m fuser_method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdid not find fuser method for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fuser_method\n",
      "\u001b[0;31mAssertionError\u001b[0m: did not find fuser method for: (<class 'torchvision.ops.misc.Conv2dNormActivation'>, <class 'torch.nn.modules.container.Sequential'>, <class 'torch.nn.modules.container.Sequential'>) "
     ]
    }
   ],
   "source": [
    "from torch.quantization import fuse_modules\n",
    "\n",
    "# Fuse Conv2d + BatchNorm2d + ReLU layers\n",
    "def fuse_efficientnet(model):\n",
    "    for module_name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Sequential):\n",
    "            fuse_modules(module, ['0', '1', '2'], inplace=True)  # Fuse Conv2d, BatchNorm2d, ReLU\n",
    "    return model\n",
    "\n",
    "model = fuse_efficientnet(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Quantization - quantizes weights only, not useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_601977/4011583075.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('fine_tuned_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Size: 17.32 MB\n",
      "Quantized Model Size: 16.58 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Quantized Model: 100%|██████████| 625/625 [02:07<00:00,  4.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for the first 10 images:\n",
      "Image 1: Predicted Class = 130\n",
      "Image 2: Predicted Class = 190\n",
      "Image 3: Predicted Class = 178\n",
      "Image 4: Predicted Class = 159\n",
      "Image 5: Predicted Class = 26\n",
      "Image 6: Predicted Class = 112\n",
      "Image 7: Predicted Class = 113\n",
      "Image 8: Predicted Class = 53\n",
      "Image 9: Predicted Class = 105\n",
      "Image 10: Predicted Class = 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.int64(130),\n",
       " np.int64(190),\n",
       " np.int64(178),\n",
       " np.int64(159),\n",
       " np.int64(26),\n",
       " np.int64(112),\n",
       " np.int64(113),\n",
       " np.int64(53),\n",
       " np.int64(105),\n",
       " np.int64(78),\n",
       " np.int64(93),\n",
       " np.int64(114),\n",
       " np.int64(36),\n",
       " np.int64(8),\n",
       " np.int64(54),\n",
       " np.int64(37),\n",
       " np.int64(41),\n",
       " np.int64(118),\n",
       " np.int64(66),\n",
       " np.int64(153),\n",
       " np.int64(83),\n",
       " np.int64(52),\n",
       " np.int64(41),\n",
       " np.int64(170),\n",
       " np.int64(191),\n",
       " np.int64(114),\n",
       " np.int64(189),\n",
       " np.int64(2),\n",
       " np.int64(30),\n",
       " np.int64(98),\n",
       " np.int64(74),\n",
       " np.int64(122),\n",
       " np.int64(184),\n",
       " np.int64(6),\n",
       " np.int64(160),\n",
       " np.int64(156),\n",
       " np.int64(119),\n",
       " np.int64(27),\n",
       " np.int64(191),\n",
       " np.int64(57),\n",
       " np.int64(105),\n",
       " np.int64(103),\n",
       " np.int64(4),\n",
       " np.int64(78),\n",
       " np.int64(4),\n",
       " np.int64(20),\n",
       " np.int64(76),\n",
       " np.int64(94),\n",
       " np.int64(72),\n",
       " np.int64(1),\n",
       " np.int64(161),\n",
       " np.int64(71),\n",
       " np.int64(34),\n",
       " np.int64(193),\n",
       " np.int64(148),\n",
       " np.int64(0),\n",
       " np.int64(83),\n",
       " np.int64(43),\n",
       " np.int64(150),\n",
       " np.int64(155),\n",
       " np.int64(58),\n",
       " np.int64(187),\n",
       " np.int64(23),\n",
       " np.int64(44),\n",
       " np.int64(150),\n",
       " np.int64(90),\n",
       " np.int64(184),\n",
       " np.int64(50),\n",
       " np.int64(34),\n",
       " np.int64(35),\n",
       " np.int64(141),\n",
       " np.int64(164),\n",
       " np.int64(161),\n",
       " np.int64(189),\n",
       " np.int64(74),\n",
       " np.int64(141),\n",
       " np.int64(82),\n",
       " np.int64(74),\n",
       " np.int64(56),\n",
       " np.int64(117),\n",
       " np.int64(191),\n",
       " np.int64(31),\n",
       " np.int64(58),\n",
       " np.int64(34),\n",
       " np.int64(96),\n",
       " np.int64(55),\n",
       " np.int64(110),\n",
       " np.int64(113),\n",
       " np.int64(110),\n",
       " np.int64(14),\n",
       " np.int64(58),\n",
       " np.int64(100),\n",
       " np.int64(39),\n",
       " np.int64(23),\n",
       " np.int64(78),\n",
       " np.int64(8),\n",
       " np.int64(24),\n",
       " np.int64(92),\n",
       " np.int64(125),\n",
       " np.int64(48),\n",
       " np.int64(123),\n",
       " np.int64(99),\n",
       " np.int64(119),\n",
       " np.int64(14),\n",
       " np.int64(145),\n",
       " np.int64(61),\n",
       " np.int64(86),\n",
       " np.int64(26),\n",
       " np.int64(186),\n",
       " np.int64(72),\n",
       " np.int64(22),\n",
       " np.int64(66),\n",
       " np.int64(24),\n",
       " np.int64(184),\n",
       " np.int64(107),\n",
       " np.int64(160),\n",
       " np.int64(74),\n",
       " np.int64(135),\n",
       " np.int64(125),\n",
       " np.int64(165),\n",
       " np.int64(54),\n",
       " np.int64(193),\n",
       " np.int64(173),\n",
       " np.int64(25),\n",
       " np.int64(68),\n",
       " np.int64(100),\n",
       " np.int64(61),\n",
       " np.int64(61),\n",
       " np.int64(89),\n",
       " np.int64(117),\n",
       " np.int64(162),\n",
       " np.int64(51),\n",
       " np.int64(120),\n",
       " np.int64(96),\n",
       " np.int64(28),\n",
       " np.int64(67),\n",
       " np.int64(143),\n",
       " np.int64(130),\n",
       " np.int64(48),\n",
       " np.int64(88),\n",
       " np.int64(125),\n",
       " np.int64(50),\n",
       " np.int64(76),\n",
       " np.int64(69),\n",
       " np.int64(150),\n",
       " np.int64(118),\n",
       " np.int64(149),\n",
       " np.int64(66),\n",
       " np.int64(165),\n",
       " np.int64(87),\n",
       " np.int64(33),\n",
       " np.int64(5),\n",
       " np.int64(27),\n",
       " np.int64(143),\n",
       " np.int64(17),\n",
       " np.int64(126),\n",
       " np.int64(4),\n",
       " np.int64(51),\n",
       " np.int64(32),\n",
       " np.int64(172),\n",
       " np.int64(110),\n",
       " np.int64(19),\n",
       " np.int64(29),\n",
       " np.int64(38),\n",
       " np.int64(81),\n",
       " np.int64(5),\n",
       " np.int64(117),\n",
       " np.int64(21),\n",
       " np.int64(189),\n",
       " np.int64(164),\n",
       " np.int64(7),\n",
       " np.int64(84),\n",
       " np.int64(111),\n",
       " np.int64(105),\n",
       " np.int64(188),\n",
       " np.int64(180),\n",
       " np.int64(73),\n",
       " np.int64(160),\n",
       " np.int64(35),\n",
       " np.int64(128),\n",
       " np.int64(141),\n",
       " np.int64(109),\n",
       " np.int64(46),\n",
       " np.int64(28),\n",
       " np.int64(29),\n",
       " np.int64(98),\n",
       " np.int64(57),\n",
       " np.int64(103),\n",
       " np.int64(34),\n",
       " np.int64(115),\n",
       " np.int64(173),\n",
       " np.int64(43),\n",
       " np.int64(144),\n",
       " np.int64(135),\n",
       " np.int64(162),\n",
       " np.int64(81),\n",
       " np.int64(107),\n",
       " np.int64(188),\n",
       " np.int64(37),\n",
       " np.int64(82),\n",
       " np.int64(198),\n",
       " np.int64(154),\n",
       " np.int64(111),\n",
       " np.int64(145),\n",
       " np.int64(153),\n",
       " np.int64(74),\n",
       " np.int64(126),\n",
       " np.int64(135),\n",
       " np.int64(104),\n",
       " np.int64(55),\n",
       " np.int64(173),\n",
       " np.int64(41),\n",
       " np.int64(109),\n",
       " np.int64(82),\n",
       " np.int64(167),\n",
       " np.int64(54),\n",
       " np.int64(19),\n",
       " np.int64(24),\n",
       " np.int64(99),\n",
       " np.int64(198),\n",
       " np.int64(62),\n",
       " np.int64(77),\n",
       " np.int64(159),\n",
       " np.int64(112),\n",
       " np.int64(52),\n",
       " np.int64(39),\n",
       " np.int64(72),\n",
       " np.int64(95),\n",
       " np.int64(22),\n",
       " np.int64(193),\n",
       " np.int64(45),\n",
       " np.int64(167),\n",
       " np.int64(101),\n",
       " np.int64(17),\n",
       " np.int64(154),\n",
       " np.int64(70),\n",
       " np.int64(117),\n",
       " np.int64(193),\n",
       " np.int64(154),\n",
       " np.int64(126),\n",
       " np.int64(46),\n",
       " np.int64(18),\n",
       " np.int64(60),\n",
       " np.int64(165),\n",
       " np.int64(45),\n",
       " np.int64(67),\n",
       " np.int64(55),\n",
       " np.int64(39),\n",
       " np.int64(73),\n",
       " np.int64(78),\n",
       " np.int64(22),\n",
       " np.int64(113),\n",
       " np.int64(153),\n",
       " np.int64(167),\n",
       " np.int64(4),\n",
       " np.int64(164),\n",
       " np.int64(176),\n",
       " np.int64(9),\n",
       " np.int64(57),\n",
       " np.int64(121),\n",
       " np.int64(0),\n",
       " np.int64(61),\n",
       " np.int64(92),\n",
       " np.int64(184),\n",
       " np.int64(81),\n",
       " np.int64(6),\n",
       " np.int64(25),\n",
       " np.int64(196),\n",
       " np.int64(29),\n",
       " np.int64(24),\n",
       " np.int64(92),\n",
       " np.int64(58),\n",
       " np.int64(118),\n",
       " np.int64(1),\n",
       " np.int64(67),\n",
       " np.int64(126),\n",
       " np.int64(122),\n",
       " np.int64(149),\n",
       " np.int64(26),\n",
       " np.int64(174),\n",
       " np.int64(67),\n",
       " np.int64(156),\n",
       " np.int64(37),\n",
       " np.int64(138),\n",
       " np.int64(57),\n",
       " np.int64(28),\n",
       " np.int64(91),\n",
       " np.int64(177),\n",
       " np.int64(81),\n",
       " np.int64(153),\n",
       " np.int64(47),\n",
       " np.int64(188),\n",
       " np.int64(111),\n",
       " np.int64(160),\n",
       " np.int64(93),\n",
       " np.int64(167),\n",
       " np.int64(164),\n",
       " np.int64(184),\n",
       " np.int64(13),\n",
       " np.int64(115),\n",
       " np.int64(116),\n",
       " np.int64(44),\n",
       " np.int64(109),\n",
       " np.int64(176),\n",
       " np.int64(25),\n",
       " np.int64(24),\n",
       " np.int64(177),\n",
       " np.int64(81),\n",
       " np.int64(20),\n",
       " np.int64(170),\n",
       " np.int64(39),\n",
       " np.int64(189),\n",
       " np.int64(137),\n",
       " np.int64(96),\n",
       " np.int64(191),\n",
       " np.int64(14),\n",
       " np.int64(177),\n",
       " np.int64(15),\n",
       " np.int64(151),\n",
       " np.int64(58),\n",
       " np.int64(94),\n",
       " np.int64(59),\n",
       " np.int64(115),\n",
       " np.int64(143),\n",
       " np.int64(28),\n",
       " np.int64(1),\n",
       " np.int64(129),\n",
       " np.int64(121),\n",
       " np.int64(70),\n",
       " np.int64(152),\n",
       " np.int64(11),\n",
       " np.int64(108),\n",
       " np.int64(85),\n",
       " np.int64(199),\n",
       " np.int64(66),\n",
       " np.int64(193),\n",
       " np.int64(81),\n",
       " np.int64(133),\n",
       " np.int64(175),\n",
       " np.int64(85),\n",
       " np.int64(178),\n",
       " np.int64(101),\n",
       " np.int64(197),\n",
       " np.int64(147),\n",
       " np.int64(185),\n",
       " np.int64(10),\n",
       " np.int64(184),\n",
       " np.int64(88),\n",
       " np.int64(60),\n",
       " np.int64(84),\n",
       " np.int64(125),\n",
       " np.int64(123),\n",
       " np.int64(54),\n",
       " np.int64(111),\n",
       " np.int64(39),\n",
       " np.int64(73),\n",
       " np.int64(98),\n",
       " np.int64(56),\n",
       " np.int64(11),\n",
       " np.int64(188),\n",
       " np.int64(165),\n",
       " np.int64(125),\n",
       " np.int64(13),\n",
       " np.int64(173),\n",
       " np.int64(122),\n",
       " np.int64(103),\n",
       " np.int64(181),\n",
       " np.int64(54),\n",
       " np.int64(99),\n",
       " np.int64(167),\n",
       " np.int64(70),\n",
       " np.int64(109),\n",
       " np.int64(189),\n",
       " np.int64(126),\n",
       " np.int64(54),\n",
       " np.int64(166),\n",
       " np.int64(118),\n",
       " np.int64(141),\n",
       " np.int64(170),\n",
       " np.int64(69),\n",
       " np.int64(150),\n",
       " np.int64(37),\n",
       " np.int64(19),\n",
       " np.int64(100),\n",
       " np.int64(28),\n",
       " np.int64(28),\n",
       " np.int64(12),\n",
       " np.int64(22),\n",
       " np.int64(25),\n",
       " np.int64(129),\n",
       " np.int64(112),\n",
       " np.int64(61),\n",
       " np.int64(139),\n",
       " np.int64(9),\n",
       " np.int64(114),\n",
       " np.int64(138),\n",
       " np.int64(8),\n",
       " np.int64(178),\n",
       " np.int64(127),\n",
       " np.int64(117),\n",
       " np.int64(107),\n",
       " np.int64(81),\n",
       " np.int64(83),\n",
       " np.int64(20),\n",
       " np.int64(85),\n",
       " np.int64(195),\n",
       " np.int64(66),\n",
       " np.int64(196),\n",
       " np.int64(170),\n",
       " np.int64(130),\n",
       " np.int64(169),\n",
       " np.int64(93),\n",
       " np.int64(175),\n",
       " np.int64(6),\n",
       " np.int64(35),\n",
       " np.int64(90),\n",
       " np.int64(133),\n",
       " np.int64(93),\n",
       " np.int64(123),\n",
       " np.int64(4),\n",
       " np.int64(7),\n",
       " np.int64(183),\n",
       " np.int64(73),\n",
       " np.int64(134),\n",
       " np.int64(15),\n",
       " np.int64(174),\n",
       " np.int64(58),\n",
       " np.int64(41),\n",
       " np.int64(126),\n",
       " np.int64(38),\n",
       " np.int64(116),\n",
       " np.int64(78),\n",
       " np.int64(11),\n",
       " np.int64(154),\n",
       " np.int64(27),\n",
       " np.int64(143),\n",
       " np.int64(112),\n",
       " np.int64(186),\n",
       " np.int64(98),\n",
       " np.int64(157),\n",
       " np.int64(155),\n",
       " np.int64(6),\n",
       " np.int64(5),\n",
       " np.int64(8),\n",
       " np.int64(25),\n",
       " np.int64(88),\n",
       " np.int64(90),\n",
       " np.int64(199),\n",
       " np.int64(165),\n",
       " np.int64(112),\n",
       " np.int64(154),\n",
       " np.int64(147),\n",
       " np.int64(160),\n",
       " np.int64(161),\n",
       " np.int64(133),\n",
       " np.int64(162),\n",
       " np.int64(120),\n",
       " np.int64(54),\n",
       " np.int64(149),\n",
       " np.int64(193),\n",
       " np.int64(48),\n",
       " np.int64(4),\n",
       " np.int64(113),\n",
       " np.int64(160),\n",
       " np.int64(113),\n",
       " np.int64(180),\n",
       " np.int64(147),\n",
       " np.int64(134),\n",
       " np.int64(130),\n",
       " np.int64(4),\n",
       " np.int64(147),\n",
       " np.int64(8),\n",
       " np.int64(199),\n",
       " np.int64(113),\n",
       " np.int64(39),\n",
       " np.int64(122),\n",
       " np.int64(90),\n",
       " np.int64(55),\n",
       " np.int64(29),\n",
       " np.int64(117),\n",
       " np.int64(194),\n",
       " np.int64(6),\n",
       " np.int64(192),\n",
       " np.int64(25),\n",
       " np.int64(119),\n",
       " np.int64(14),\n",
       " np.int64(175),\n",
       " np.int64(10),\n",
       " np.int64(153),\n",
       " np.int64(58),\n",
       " np.int64(102),\n",
       " np.int64(141),\n",
       " np.int64(59),\n",
       " np.int64(141),\n",
       " np.int64(88),\n",
       " np.int64(20),\n",
       " np.int64(20),\n",
       " np.int64(193),\n",
       " np.int64(85),\n",
       " np.int64(36),\n",
       " np.int64(31),\n",
       " np.int64(128),\n",
       " np.int64(119),\n",
       " np.int64(194),\n",
       " np.int64(183),\n",
       " np.int64(42),\n",
       " np.int64(199),\n",
       " np.int64(98),\n",
       " np.int64(193),\n",
       " np.int64(80),\n",
       " np.int64(101),\n",
       " np.int64(193),\n",
       " np.int64(120),\n",
       " np.int64(35),\n",
       " np.int64(121),\n",
       " np.int64(59),\n",
       " np.int64(178),\n",
       " np.int64(151),\n",
       " np.int64(37),\n",
       " np.int64(147),\n",
       " np.int64(85),\n",
       " np.int64(68),\n",
       " np.int64(118),\n",
       " np.int64(145),\n",
       " np.int64(13),\n",
       " np.int64(21),\n",
       " np.int64(14),\n",
       " np.int64(18),\n",
       " np.int64(20),\n",
       " np.int64(141),\n",
       " np.int64(182),\n",
       " np.int64(126),\n",
       " np.int64(107),\n",
       " np.int64(191),\n",
       " np.int64(74),\n",
       " np.int64(164),\n",
       " np.int64(43),\n",
       " np.int64(50),\n",
       " np.int64(68),\n",
       " np.int64(190),\n",
       " np.int64(162),\n",
       " np.int64(115),\n",
       " np.int64(79),\n",
       " np.int64(175),\n",
       " np.int64(154),\n",
       " np.int64(42),\n",
       " np.int64(194),\n",
       " np.int64(110),\n",
       " np.int64(8),\n",
       " np.int64(61),\n",
       " np.int64(5),\n",
       " np.int64(185),\n",
       " np.int64(134),\n",
       " np.int64(46),\n",
       " np.int64(58),\n",
       " np.int64(86),\n",
       " np.int64(165),\n",
       " np.int64(187),\n",
       " np.int64(1),\n",
       " np.int64(118),\n",
       " np.int64(147),\n",
       " np.int64(74),\n",
       " np.int64(173),\n",
       " np.int64(11),\n",
       " np.int64(133),\n",
       " np.int64(168),\n",
       " np.int64(199),\n",
       " np.int64(151),\n",
       " np.int64(194),\n",
       " np.int64(45),\n",
       " np.int64(109),\n",
       " np.int64(183),\n",
       " np.int64(169),\n",
       " np.int64(17),\n",
       " np.int64(69),\n",
       " np.int64(29),\n",
       " np.int64(52),\n",
       " np.int64(94),\n",
       " np.int64(126),\n",
       " np.int64(106),\n",
       " np.int64(180),\n",
       " np.int64(39),\n",
       " np.int64(35),\n",
       " np.int64(76),\n",
       " np.int64(6),\n",
       " np.int64(55),\n",
       " np.int64(103),\n",
       " np.int64(124),\n",
       " np.int64(50),\n",
       " np.int64(33),\n",
       " np.int64(85),\n",
       " np.int64(47),\n",
       " np.int64(37),\n",
       " np.int64(21),\n",
       " np.int64(174),\n",
       " np.int64(18),\n",
       " np.int64(191),\n",
       " np.int64(78),\n",
       " np.int64(123),\n",
       " np.int64(119),\n",
       " np.int64(25),\n",
       " np.int64(111),\n",
       " np.int64(181),\n",
       " np.int64(52),\n",
       " np.int64(20),\n",
       " np.int64(49),\n",
       " np.int64(114),\n",
       " np.int64(5),\n",
       " np.int64(81),\n",
       " np.int64(194),\n",
       " np.int64(129),\n",
       " np.int64(136),\n",
       " np.int64(2),\n",
       " np.int64(114),\n",
       " np.int64(5),\n",
       " np.int64(42),\n",
       " np.int64(188),\n",
       " np.int64(14),\n",
       " np.int64(57),\n",
       " np.int64(117),\n",
       " np.int64(170),\n",
       " np.int64(37),\n",
       " np.int64(41),\n",
       " np.int64(98),\n",
       " np.int64(177),\n",
       " np.int64(156),\n",
       " np.int64(144),\n",
       " np.int64(47),\n",
       " np.int64(199),\n",
       " np.int64(91),\n",
       " np.int64(175),\n",
       " np.int64(193),\n",
       " np.int64(74),\n",
       " np.int64(109),\n",
       " np.int64(109),\n",
       " np.int64(93),\n",
       " np.int64(81),\n",
       " np.int64(185),\n",
       " np.int64(74),\n",
       " np.int64(166),\n",
       " np.int64(189),\n",
       " np.int64(143),\n",
       " np.int64(37),\n",
       " np.int64(42),\n",
       " np.int64(143),\n",
       " np.int64(136),\n",
       " np.int64(108),\n",
       " np.int64(184),\n",
       " np.int64(193),\n",
       " np.int64(194),\n",
       " np.int64(138),\n",
       " np.int64(199),\n",
       " np.int64(141),\n",
       " np.int64(51),\n",
       " np.int64(44),\n",
       " np.int64(144),\n",
       " np.int64(99),\n",
       " np.int64(4),\n",
       " np.int64(155),\n",
       " np.int64(39),\n",
       " np.int64(173),\n",
       " np.int64(150),\n",
       " np.int64(10),\n",
       " np.int64(4),\n",
       " np.int64(153),\n",
       " np.int64(186),\n",
       " np.int64(42),\n",
       " np.int64(106),\n",
       " np.int64(46),\n",
       " np.int64(98),\n",
       " np.int64(164),\n",
       " np.int64(138),\n",
       " np.int64(191),\n",
       " np.int64(198),\n",
       " np.int64(133),\n",
       " np.int64(160),\n",
       " np.int64(169),\n",
       " np.int64(57),\n",
       " np.int64(155),\n",
       " np.int64(68),\n",
       " np.int64(104),\n",
       " np.int64(1),\n",
       " np.int64(178),\n",
       " np.int64(166),\n",
       " np.int64(189),\n",
       " np.int64(181),\n",
       " np.int64(141),\n",
       " np.int64(51),\n",
       " np.int64(165),\n",
       " np.int64(165),\n",
       " np.int64(193),\n",
       " np.int64(79),\n",
       " np.int64(82),\n",
       " np.int64(60),\n",
       " np.int64(128),\n",
       " np.int64(115),\n",
       " np.int64(172),\n",
       " np.int64(111),\n",
       " np.int64(155),\n",
       " np.int64(175),\n",
       " np.int64(60),\n",
       " np.int64(76),\n",
       " np.int64(25),\n",
       " np.int64(115),\n",
       " np.int64(73),\n",
       " np.int64(74),\n",
       " np.int64(70),\n",
       " np.int64(174),\n",
       " np.int64(134),\n",
       " np.int64(10),\n",
       " np.int64(108),\n",
       " np.int64(97),\n",
       " np.int64(86),\n",
       " np.int64(72),\n",
       " np.int64(89),\n",
       " np.int64(167),\n",
       " np.int64(32),\n",
       " np.int64(130),\n",
       " np.int64(28),\n",
       " np.int64(20),\n",
       " np.int64(129),\n",
       " np.int64(2),\n",
       " np.int64(23),\n",
       " np.int64(169),\n",
       " np.int64(193),\n",
       " np.int64(55),\n",
       " np.int64(166),\n",
       " np.int64(108),\n",
       " np.int64(117),\n",
       " np.int64(28),\n",
       " np.int64(117),\n",
       " np.int64(36),\n",
       " np.int64(122),\n",
       " np.int64(36),\n",
       " np.int64(101),\n",
       " np.int64(20),\n",
       " np.int64(81),\n",
       " np.int64(144),\n",
       " np.int64(59),\n",
       " np.int64(95),\n",
       " np.int64(144),\n",
       " np.int64(190),\n",
       " np.int64(81),\n",
       " np.int64(44),\n",
       " np.int64(0),\n",
       " np.int64(174),\n",
       " np.int64(114),\n",
       " np.int64(65),\n",
       " np.int64(159),\n",
       " np.int64(42),\n",
       " np.int64(196),\n",
       " np.int64(9),\n",
       " np.int64(43),\n",
       " np.int64(145),\n",
       " np.int64(81),\n",
       " np.int64(51),\n",
       " np.int64(18),\n",
       " np.int64(58),\n",
       " np.int64(153),\n",
       " np.int64(85),\n",
       " np.int64(153),\n",
       " np.int64(1),\n",
       " np.int64(189),\n",
       " np.int64(1),\n",
       " np.int64(121),\n",
       " np.int64(98),\n",
       " np.int64(111),\n",
       " np.int64(98),\n",
       " np.int64(85),\n",
       " np.int64(67),\n",
       " np.int64(150),\n",
       " np.int64(181),\n",
       " np.int64(180),\n",
       " np.int64(60),\n",
       " np.int64(31),\n",
       " np.int64(57),\n",
       " np.int64(128),\n",
       " np.int64(141),\n",
       " np.int64(193),\n",
       " np.int64(171),\n",
       " np.int64(59),\n",
       " np.int64(84),\n",
       " np.int64(147),\n",
       " np.int64(148),\n",
       " np.int64(145),\n",
       " np.int64(190),\n",
       " np.int64(81),\n",
       " np.int64(149),\n",
       " np.int64(36),\n",
       " np.int64(105),\n",
       " np.int64(135),\n",
       " np.int64(9),\n",
       " np.int64(93),\n",
       " np.int64(39),\n",
       " np.int64(67),\n",
       " np.int64(81),\n",
       " np.int64(71),\n",
       " np.int64(57),\n",
       " np.int64(154),\n",
       " np.int64(110),\n",
       " np.int64(12),\n",
       " np.int64(78),\n",
       " np.int64(125),\n",
       " np.int64(35),\n",
       " np.int64(58),\n",
       " np.int64(27),\n",
       " np.int64(99),\n",
       " np.int64(52),\n",
       " np.int64(13),\n",
       " np.int64(74),\n",
       " np.int64(149),\n",
       " np.int64(37),\n",
       " np.int64(115),\n",
       " np.int64(142),\n",
       " np.int64(104),\n",
       " np.int64(19),\n",
       " np.int64(104),\n",
       " np.int64(83),\n",
       " np.int64(16),\n",
       " np.int64(119),\n",
       " np.int64(146),\n",
       " np.int64(101),\n",
       " np.int64(170),\n",
       " np.int64(149),\n",
       " np.int64(43),\n",
       " np.int64(97),\n",
       " np.int64(73),\n",
       " np.int64(87),\n",
       " np.int64(145),\n",
       " np.int64(23),\n",
       " np.int64(143),\n",
       " np.int64(51),\n",
       " np.int64(13),\n",
       " np.int64(180),\n",
       " np.int64(124),\n",
       " np.int64(93),\n",
       " np.int64(127),\n",
       " np.int64(165),\n",
       " np.int64(161),\n",
       " np.int64(119),\n",
       " np.int64(47),\n",
       " np.int64(2),\n",
       " np.int64(104),\n",
       " np.int64(93),\n",
       " np.int64(15),\n",
       " np.int64(31),\n",
       " np.int64(147),\n",
       " np.int64(89),\n",
       " np.int64(31),\n",
       " np.int64(54),\n",
       " np.int64(103),\n",
       " np.int64(79),\n",
       " np.int64(58),\n",
       " np.int64(98),\n",
       " np.int64(144),\n",
       " np.int64(90),\n",
       " np.int64(1),\n",
       " np.int64(15),\n",
       " np.int64(74),\n",
       " np.int64(37),\n",
       " np.int64(66),\n",
       " np.int64(0),\n",
       " np.int64(43),\n",
       " np.int64(128),\n",
       " np.int64(164),\n",
       " np.int64(36),\n",
       " np.int64(180),\n",
       " np.int64(35),\n",
       " np.int64(118),\n",
       " np.int64(183),\n",
       " np.int64(191),\n",
       " np.int64(94),\n",
       " np.int64(56),\n",
       " np.int64(98),\n",
       " np.int64(14),\n",
       " np.int64(103),\n",
       " np.int64(15),\n",
       " np.int64(1),\n",
       " np.int64(86),\n",
       " np.int64(30),\n",
       " np.int64(93),\n",
       " np.int64(131),\n",
       " np.int64(161),\n",
       " np.int64(54),\n",
       " np.int64(112),\n",
       " np.int64(155),\n",
       " np.int64(39),\n",
       " np.int64(105),\n",
       " np.int64(11),\n",
       " np.int64(179),\n",
       " np.int64(7),\n",
       " np.int64(37),\n",
       " np.int64(193),\n",
       " np.int64(83),\n",
       " np.int64(81),\n",
       " np.int64(111),\n",
       " np.int64(15),\n",
       " np.int64(110),\n",
       " np.int64(58),\n",
       " np.int64(52),\n",
       " np.int64(193),\n",
       " np.int64(28),\n",
       " np.int64(136),\n",
       " np.int64(142),\n",
       " np.int64(26),\n",
       " np.int64(38),\n",
       " np.int64(133),\n",
       " np.int64(12),\n",
       " np.int64(145),\n",
       " np.int64(76),\n",
       " np.int64(42),\n",
       " np.int64(74),\n",
       " np.int64(10),\n",
       " np.int64(156),\n",
       " np.int64(34),\n",
       " np.int64(169),\n",
       " np.int64(1),\n",
       " np.int64(83),\n",
       " np.int64(78),\n",
       " np.int64(8),\n",
       " np.int64(130),\n",
       " np.int64(56),\n",
       " np.int64(41),\n",
       " np.int64(126),\n",
       " np.int64(59),\n",
       " np.int64(18),\n",
       " np.int64(111),\n",
       " np.int64(141),\n",
       " np.int64(59),\n",
       " np.int64(187),\n",
       " np.int64(128),\n",
       " np.int64(66),\n",
       " np.int64(185),\n",
       " np.int64(12),\n",
       " np.int64(51),\n",
       " np.int64(66),\n",
       " np.int64(170),\n",
       " np.int64(28),\n",
       " np.int64(107),\n",
       " np.int64(155),\n",
       " np.int64(61),\n",
       " np.int64(5),\n",
       " np.int64(190),\n",
       " np.int64(20),\n",
       " np.int64(104),\n",
       " np.int64(181),\n",
       " np.int64(83),\n",
       " np.int64(193),\n",
       " np.int64(58),\n",
       " np.int64(199),\n",
       " np.int64(60),\n",
       " np.int64(43),\n",
       " np.int64(15),\n",
       " np.int64(113),\n",
       " np.int64(111),\n",
       " np.int64(81),\n",
       " np.int64(56),\n",
       " np.int64(98),\n",
       " np.int64(77),\n",
       " np.int64(18),\n",
       " np.int64(28),\n",
       " np.int64(188),\n",
       " np.int64(124),\n",
       " np.int64(181),\n",
       " np.int64(167),\n",
       " np.int64(115),\n",
       " np.int64(100),\n",
       " np.int64(136),\n",
       " np.int64(9),\n",
       " np.int64(43),\n",
       " np.int64(147),\n",
       " np.int64(128),\n",
       " np.int64(108),\n",
       " np.int64(199),\n",
       " np.int64(31),\n",
       " np.int64(4),\n",
       " np.int64(32),\n",
       " np.int64(154),\n",
       " np.int64(192),\n",
       " np.int64(129),\n",
       " np.int64(50),\n",
       " np.int64(112),\n",
       " np.int64(191),\n",
       " np.int64(72),\n",
       " np.int64(57),\n",
       " np.int64(60),\n",
       " np.int64(43),\n",
       " np.int64(69),\n",
       " np.int64(128),\n",
       " np.int64(51),\n",
       " np.int64(10),\n",
       " np.int64(8),\n",
       " np.int64(31),\n",
       " np.int64(160),\n",
       " np.int64(98),\n",
       " np.int64(136),\n",
       " np.int64(18),\n",
       " np.int64(167),\n",
       " np.int64(1),\n",
       " np.int64(106),\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Step 1: Load the Fine-Tuned Model\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "model._fc = torch.nn.Linear(model._fc.in_features, 200)  # Ensure 200 output classes\n",
    "model.load_state_dict(torch.load('fine_tuned_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Quantize the Model (Dynamic Quantization)\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model,  # Model to quantize\n",
    "    {torch.nn.Linear},  # Layers to quantize (e.g., Linear layers)\n",
    "    dtype=torch.qint8  # Quantization type\n",
    ")\n",
    "\n",
    "# Print model size comparison\n",
    "original_size = torch.save(model.state_dict(), 'temp.pth') or os.path.getsize('temp.pth')\n",
    "quantized_size = torch.save(quantized_model.state_dict(), 'temp_quantized.pth') or os.path.getsize('temp_quantized.pth')\n",
    "os.remove('temp.pth')\n",
    "os.remove('temp_quantized.pth')\n",
    "\n",
    "print(f\"Original Model Size: {original_size / 1e6:.2f} MB\")\n",
    "print(f\"Quantized Model Size: {quantized_size / 1e6:.2f} MB\")\n",
    "\n",
    "# Step 3: Test the Quantized Model\n",
    "def evaluate(model, test_loader, device, class_names=None):\n",
    "    model.to('cpu')  # Force the model to use CPU for quantized operations\n",
    "    model.eval()\n",
    "    predictions = []  # To store predictions for all images\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing Quantized Model\"):\n",
    "            images, _ = batch  # Ignore file paths, we don't need them for evaluation\n",
    "            images = images.to('cpu')  # Move the images to CPU\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            # If class names are provided, map the indices to class names\n",
    "            if class_names:\n",
    "                predicted_classes = [class_names[idx] for idx in predicted]\n",
    "                predictions.extend(predicted_classes)\n",
    "            else:\n",
    "                predictions.extend(predicted.cpu().numpy())  # If no class names, just return indices\n",
    "\n",
    "    # Display some of the predictions\n",
    "    print(\"Predictions for the first 10 images:\")\n",
    "    for i, prediction in enumerate(predictions[:10]):\n",
    "        print(f\"Image {i+1}: Predicted Class = {prediction}\")\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming test_loader is already defined for your test dataset\n",
    "\n",
    "#######\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#  for the above line\n",
    "# The error you're encountering suggests that the quantized model is trying to run on a CUDA device, but the operation quantized::linear_dynamic doesn't support the CUDA backend. \n",
    "# This is a known limitation with PyTorch quantization: certain operations, specifically linear_dynamic for quantized models, may not have support for execution on GPUs (CUDA). \n",
    "# This operation is typically supported only on CPU.\n",
    "######\n",
    "evaluate(quantized_model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = datasets.ImageFolder(root=\"/home/btp_9/EfficientNet/tiny-imagenet-another/tiny-imagenet-200/val_restructured\", transform=transform)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the quantized EfficientNet on Tiny ImageNet: 46.10%\n"
     ]
    }
   ],
   "source": [
    "def test(model, dataloader):\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate quantized model on Tiny ImageNet validation dataset\n",
    "accuracy = test(model, val_loader)\n",
    "print(f'Accuracy of the quantized EfficientNet on Tiny ImageNet: {accuracy:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_class_names(file_path):\n",
    "    class_ids = []  # List of class IDs (e.g., n00001740)\n",
    "    class_names = {}  # Mapping from class ID to class name\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            class_id = parts[0]\n",
    "            class_name = parts[1]\n",
    "            class_ids.append(class_id)  # Store the class ID\n",
    "            class_names[class_id] = class_name  # Map class ID to class name\n",
    "    return class_ids, class_names\n",
    "\n",
    "def evaluate(model, test_loader, device, class_ids, class_names):\n",
    "    model.to('cpu')  # Force the model to use CPU for quantized operations\n",
    "    model.eval()\n",
    "    predictions = []  # To store predictions for all images\n",
    "    predictions_class_ids = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing Quantized Model\"):\n",
    "            images, _ = batch  # Ignore file paths, we don't need them for evaluation\n",
    "            images = images.to('cpu')  # Move the images to CPU\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            # Map predicted class indices to class IDs\n",
    "            predicted_class_ids = [class_ids[idx] for idx in predicted.cpu().numpy()]\n",
    "            predicted_classes = [class_names[class_id] for class_id in predicted_class_ids]\n",
    "            predictions.extend(predicted_classes)\n",
    "            predictions_class_ids.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Display some of the predictions\n",
    "    print(\"Predictions for the first 10 images:\")\n",
    "    for i, prediction in enumerate(predictions[:10]):\n",
    "        print(f\"Image {i+1}: Predicted Class = {prediction}\")\n",
    "\n",
    "    for i, prediction in enumerate(predictions_class_ids[:10]):\n",
    "        print(f\"Image {i+1}: Predicted Class = {prediction}\")\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Quantized Model: 100%|██████████| 625/625 [02:07<00:00,  4.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for the first 10 images:\n",
      "Image 1: Predicted Class = penetration\n",
      "Image 2: Predicted Class = male orgasm\n",
      "Image 3: Predicted Class = exodus, hegira, hejira\n",
      "Image 4: Predicted Class = retreat\n",
      "Image 5: Predicted Class = nutrient\n",
      "Image 6: Predicted Class = arrival, reaching\n",
      "Image 7: Predicted Class = arrival\n",
      "Image 8: Predicted Class = abdominoplasty, tummy tuck\n",
      "Image 9: Predicted Class = tour de force\n",
      "Image 10: Predicted Class = reciprocity\n",
      "Image 1: Predicted Class = 130\n",
      "Image 2: Predicted Class = 190\n",
      "Image 3: Predicted Class = 178\n",
      "Image 4: Predicted Class = 159\n",
      "Image 5: Predicted Class = 26\n",
      "Image 6: Predicted Class = 112\n",
      "Image 7: Predicted Class = 113\n",
      "Image 8: Predicted Class = 53\n",
      "Image 9: Predicted Class = 105\n",
      "Image 10: Predicted Class = 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class_ids, class_names = load_class_names('/home/btp_9/EfficientNet/tiny-imagenet-another/tiny-imagenet-200/words.txt')  # Provide the correct path\n",
    "predictions = evaluate(quantized_model, test_loader, device, class_ids, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define a custom dataset\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, image_folder, annotations_file, class_names):\n",
    "        self.image_folder = image_folder\n",
    "        self.class_names = class_names\n",
    "        \n",
    "        # Read the annotations\n",
    "        with open(annotations_file, 'r') as file:\n",
    "            self.annotations = file.readlines()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the annotation for this image\n",
    "        line = self.annotations[idx].strip().split()\n",
    "        img_name = line[0]  # e.g., 'val_0.JPEG'\n",
    "        class_id = line[1]  # e.g., 'n03444034'\n",
    "        bbox = list(map(int, line[2:]))  # Bounding box: [x1, y1, x2, y2]\n",
    "        \n",
    "        # Load the image\n",
    "        img_path = os.path.join(self.image_folder, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Convert to tensor and normalize (if needed)\n",
    "        image = torch.tensor(np.array(image)).float()  # Or apply any necessary transforms\n",
    "        \n",
    "        # Map class_id to class name\n",
    "        class_name = self.class_names.get(class_id, \"Unknown\")\n",
    "        \n",
    "        return image, class_name, bbox\n",
    "\n",
    "# Load class names (from your words.txt or any other mapping)\n",
    "def load_class_names(file_path):\n",
    "    class_names = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            class_names[parts[0]] = parts[1]  # Map class ID to name\n",
    "    return class_names\n",
    "\n",
    "# Example usage\n",
    "image_folder = '/home/btp_9/EfficientNet/tiny-imagenet-another/tiny-imagenet-200/val/images'  # Replace with your actual folder path\n",
    "annotations_file = '/home/btp_9/EfficientNet/tiny-imagenet-another/tiny-imagenet-200/val/val_annotations.txt'  # Replace with the actual file path\n",
    "class_names = load_class_names('/home/btp_9/EfficientNet/tiny-imagenet-another/tiny-imagenet-200/words.txt')  # Provide the correct path to words.txt\n",
    "\n",
    "# Create the dataset\n",
    "test_dataset = CustomTestDataset(image_folder, annotations_file, class_names)\n",
    "\n",
    "# Create DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Iterate over the test loader\n",
    "for images, class_names, bbox in test_loader:\n",
    "    print(f\"Image: {images}, Class: {class_names}, Bounding Box: {bbox}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 64, 65, 4] to have 3 channels, but got 64 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[63], line 43\u001b[0m, in \u001b[0;36mcalculate_accuracy\u001b[0;34m(model, test_loader, device)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, true_class_names, true_class_ids \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m     42\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 43\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Get the class with max probability\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Compare predicted class with true class\u001b[39;00m\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/efficientnet_pytorch/model.py:314\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"EfficientNet's forward function.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m   Calls extract_features to extract features, applies final linear layer, and returns logits.\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03m    Output of this model after processing.\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# Convolution layers\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Pooling and final linear layer\u001b[39;00m\n\u001b[1;32m    316\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_avg_pooling(x)\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/efficientnet_pytorch/model.py:289\u001b[0m, in \u001b[0;36mEfficientNet.extract_features\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"use convolution layer to extract feature .\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    layer in the efficientnet model.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Stem\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swish(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bn0(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_stem\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# Blocks\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocks):\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/EfficientNet/venv311/lib/python3.11/site-packages/efficientnet_pytorch/utils.py:275\u001b[0m, in \u001b[0;36mConv2dStaticSamePadding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    274\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_padding(x)\n\u001b[0;32m--> 275\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 64, 65, 4] to have 3 channels, but got 64 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Define a custom dataset for testing (like before)\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, image_folder, annotations_file, class_names):\n",
    "        self.image_folder = image_folder\n",
    "        self.class_names = class_names\n",
    "        \n",
    "        with open(annotations_file, 'r') as file:\n",
    "            self.annotations = file.readlines()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        line = self.annotations[idx].strip().split()\n",
    "        img_name = line[0]\n",
    "        class_id = line[1]\n",
    "        bbox = list(map(int, line[2:]))\n",
    "        \n",
    "        img_path = os.path.join(self.image_folder, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        image = torch.tensor(np.array(image)).float()  # Normalize or apply transforms\n",
    "        \n",
    "        # Convert class_id to class name\n",
    "        class_name = self.class_names.get(class_id, \"Unknown\")\n",
    "        \n",
    "        return image, class_name, class_id  # Also return the true class_id for comparison\n",
    "\n",
    "# Assuming you have the model and class_names loaded\n",
    "def calculate_accuracy(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, true_class_names, true_class_ids in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the class with max probability\n",
    "\n",
    "            # Compare predicted class with true class\n",
    "            correct_predictions += (predicted == true_class_ids.to(device)).sum().item()\n",
    "            total_predictions += true_class_ids.size(0)\n",
    "    \n",
    "    accuracy = 100 * correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "# Create DataLoader for test set\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = calculate_accuracy(model, test_loader, device)\n",
    "print(f'Accuracy: {accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
